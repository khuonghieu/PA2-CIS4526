{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "import graphviz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#szDatasetPath = 'breast_cancer.csv'\n",
    "szDatasetPath = 'titanic_train.csv'\n",
    "\n",
    "#szDatasetPath = sys.argv[1]\n",
    "\n",
    "'''\n",
    "Read the data from the csv file\n",
    "listColNames[j] stores the jth column name\n",
    "listData[i][:-1] are the features of the ith example\n",
    "listData[i][-1] is the target value of the ith example\n",
    "'''\n",
    "listColNames = [] # The list of column names\n",
    "listData = [] # The list of feature vectors of all the examples\n",
    "nRow = 0\n",
    "with open(szDatasetPath) as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter=',')\n",
    "    for row in csvReader:\n",
    "        if 0 == nRow:\n",
    "            listColNames = row\n",
    "        else:\n",
    "            listData.append(row)\n",
    "        nRow += 1\n",
    "\n",
    "'''\n",
    "Scan the data and store the unique values of each column.\n",
    "listColUniqueVals[j] stores a list of unique values of the jth column\n",
    "'''\n",
    "listColUniqueVals = [[] for i in range(len(listColNames))]\n",
    "for example in listData:\n",
    "    for i in range(len(example)):\n",
    "        if example[i] not in listColUniqueVals[i]:\n",
    "            listColUniqueVals[i].append(example[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoder to use when encoding features and labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "#Encode the label array\n",
    "classCol = [row[-1] for row in listData]\n",
    "classColEncoded = le.fit_transform(classCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First_class', 'Sex', 'Age', 'SibSp', 'ParCh', 'Embarked', 'Survived']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listColNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1'],\n",
       " ['0', '1'],\n",
       " ['0', '1'],\n",
       " ['0', '1'],\n",
       " ['1', '0'],\n",
       " ['0', '1'],\n",
       " ['1', '0']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listColUniqueVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostOccur(featureColumn,element, labelCol):\n",
    "    oneOccur =0\n",
    "    zeroOccur =0\n",
    "    for i in range(len(featureColumn)):\n",
    "        if(featureColumn[i]==element):\n",
    "            if(labelCol[i]==1):\n",
    "                oneOccur+=1\n",
    "            elif(labelCol[i]==0):\n",
    "                zeroOccur+=1\n",
    "    if(oneOccur>=zeroOccur):\n",
    "        return oneOccur\n",
    "    else:\n",
    "        return zeroOccur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of feature: First_class :  0.3249299719887955\n",
      "Error rate of feature: Sex :  0.21988795518207283\n",
      "Error rate of feature: Age :  0.4061624649859944\n",
      "Error rate of feature: SibSp :  0.4061624649859944\n",
      "Error rate of feature: ParCh :  0.3851540616246498\n",
      "Error rate of feature: Embarked :  0.3837535014005602\n"
     ]
    }
   ],
   "source": [
    "# For each feature, compute the training error of a one-level decision tree\n",
    "\n",
    "#Iterate through all features\n",
    "for i in range(0,len(listColNames)-1):\n",
    "    \n",
    "    #Create a feature array\n",
    "    featureCol = [row[i] for row in listData]\n",
    "    #Total numbers of predicted labels\n",
    "    totalPredict = 0\n",
    "    #Count number of occurences of label 0 and 1 in each element of each feature\n",
    "    for element in listColUniqueVals[i]:\n",
    "        mostOccurAmount = mostOccur(featureCol,element,classColEncoded)\n",
    "        totalPredict += mostOccurAmount\n",
    "        #print(\"Element \", element, \" occured \",mostOccurAmount)\n",
    "    print(\"Error rate of feature:\",listColNames[i],\": \",1- totalPredict/len(featureCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth 1 : Error rate 0.21988795518207283\n",
      "Max depth 2 : Error rate 0.21988795518207283\n",
      "Max depth 3 : Error rate 0.21988795518207283\n",
      "Max depth 4 : Error rate 0.2184873949579832\n",
      "Max depth 5 : Error rate 0.20868347338935578\n",
      "Max depth 6 : Error rate 0.20308123249299714\n",
      "Max depth 7 : Error rate 0.20308123249299714\n",
      "Max depth 8 : Error rate 0.20308123249299714\n",
      "Max depth 9 : Error rate 0.20308123249299714\n",
      "Max depth 10 : Error rate 0.20308123249299714\n",
      "Max depth 11 : Error rate 0.20308123249299714\n",
      "Max depth 12 : Error rate 0.20308123249299714\n",
      "Max depth 13 : Error rate 0.20308123249299714\n",
      "Max depth 14 : Error rate 0.20308123249299714\n",
      "Max depth 15 : Error rate 0.20308123249299714\n",
      "Max depth 16 : Error rate 0.20308123249299714\n",
      "Max depth 17 : Error rate 0.20308123249299714\n",
      "Max depth 18 : Error rate 0.20308123249299714\n",
      "Max depth 19 : Error rate 0.20308123249299714\n",
      "Max depth 20 : Error rate 0.20308123249299714\n"
     ]
    }
   ],
   "source": [
    "# Construct a full decision tree on the dataset and compute the training error\n",
    "\n",
    "#Decision tree training on all features\n",
    "newListData = []\n",
    "for i in range(0,len(listColNames)-1):\n",
    "    #Create a feature array, then encode it for decision tree\n",
    "    featureCol = [row[i] for row in listData]\n",
    "    featureCol = le.fit_transform(featureCol)\n",
    "    newListData.append(featureCol)\n",
    "#Convert and transpose new listData\n",
    "encodedData = np.array(newListData)\n",
    "encodedData=encodedData.transpose()\n",
    "#Decision tree built with sklearn, iterating max depth from 3 to 10\n",
    "for n in range(1,21):\n",
    "    decisiontree = tree.DecisionTreeClassifier(max_depth=n,criterion='entropy')\n",
    "    decisiontree.fit(encodedData,classColEncoded)\n",
    "    print(\"Max depth\",n,\": Error rate\",1-decisiontree.score(encodedData,classColEncoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
